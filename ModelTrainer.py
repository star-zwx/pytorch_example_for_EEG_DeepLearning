from datetime import datetime

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from utils import *
from models import EEGNet, MCNN, EEGInception, TransNet
from tqdm import tqdm

from sklearn.metrics import accuracy_score, recall_score, f1_score


class ModelTrainer():
    def __init__(self, para_config_dir):
        # å…ˆå¯¹è¶…å‚æ•°åšä¸€äº›å£°æ˜
        self.para_config_dir = para_config_dir
        self.model_name = self.para_config_dir["model_name"]  # è·å–æ¨¡å‹åç§°
        self.data_name = self.para_config_dir["data_name"]  # è·å–æ•°æ®é›†åç§°
        self.data_root_path = self.para_config_dir["data_root_path"]  # è·å–æ•°æ®é›†æ ¹åœ°å€
        self.learning_rate = self.para_config_dir["learning_rate"]  # è·å–åˆå§‹å­¦ä¹ ç‡
        self.batch_size = self.para_config_dir["batch_size"]  # è·å–batch_size
        self.epochs = self.para_config_dir["epochs"]  # è·å–epochs
        self.repeat_time = self.para_config_dir["repeat_time"]
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")  # è®¾ç½®è®­ç»ƒè®¾å¤‡
        self.optimizer_name = self.para_config_dir["optimizer"]  # è·å–ä¼˜åŒ–å™¨åç§°
        self.loss_name = self.para_config_dir["loss_function"]  # è·å–æŸå¤±å‡½æ•°åç§°
        self.activation_func_name = self.para_config_dir["activation_function"]  # è·å–æ¿€æ´»å‡½æ•°åç§°
        self.early_stopping = self.para_config_dir["early_stopping"]  # è·å–æ˜¯å¦ä½¿ç”¨æ—©åœç­–ç•¥
        self.learning_rate_decay = self.para_config_dir["learning_rate_decay"]  # è·å–æ˜¯å¦é‡‡å–å­¦ä¹ ç‡è¡°å‡
        self.init_seed = self.para_config_dir["init_seed"]  # è·å–éšæœºç§å­
        self.data_split = self.para_config_dir["data_split"]  # è·å–æ•°æ®åˆ’åˆ†çš„æ¯”ä¾‹
        self.Subject_sigal = self.para_config_dir["Subject_sigal"]  # è·å–æ˜¯å¦ç”¨å•ä¸ªè¢«è¯•æˆ–è€…å…¨è¢«è¯•è®­ç»ƒ
        self.data_logs_path = self.para_config_dir["save_logs_path"]  # è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ•°æ®ä¿å­˜ä½ç½®
        self.save_model_path = self.para_config_dir["save_model_path"]  # æ¨¡å‹ä¿å­˜åœ°å€
        self.Cross_validation = self.para_config_dir["Cross_validation"]  # æ˜¯å¦é€‰æ‹©äº¤å‰éªŒè¯çš„ç­–ç•¥çš„é…ç½®
        self.EEGDataInfo = self.para_config_dir["EEGDataInfo"] # è·å–è„‘ç”µæ•°æ®çš„ä¿¡æ¯

        # å£°æ˜æ•°æ®åŠ è½½å™¨ï¼Œè®­ç»ƒé›†ï¼Œæµ‹è¯•é›†ï¼ŒéªŒè¯é›†
        self.dataLoader_train = None
        self.dataLoader_test = None
        self.dataLoader_valid = None
        self.model = None

        # è®¾ç½®éšæœºç§å­
        torch.manual_seed(self.init_seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed_all(self.init_seed)

        # è¿™é‡Œéœ€è¦ä¿®æ”¹ä¸ºå®é™…çš„æ¨¡å‹åç§°,æ³¨æ„è·¯å¾„å…³ç³»
        self.model_dict = {
            "EEGNet": EEGNet.EEGNet,
            "EEGNet2": EEGNet.EEGNet,
            "MCNN": MCNN.MCNN,
            "EEGInception": EEGInception.Inception_EEG,
            "TransNet": TransNet.TransNet,
        }  #åœ¨è¿™é‡Œæ·»åŠ éœ€è¦åœ¨ä¸Šé¢å¯¼å…¥æ¨¡å—é‚£å¯¼å…¥å¯¹åº”çš„pythonæ¨¡å—

    def data_loader(self):
        if self.Subject_sigal["sigal"] == "yes":
            all_dataset = EEGDataset(self.data_name, self.data_root_path, self.Subject_sigal["subject_name"])  # åŠ è½½æ•°æ®é›†
            train_dataset, val_dataset, test_dataset = data_split(all_dataset, self.data_split["train"],
                                                                  self.data_split["val"], self.data_split["test"])  #
            print(train_dataset, val_dataset, test_dataset)
            # åˆ’åˆ†æ•°æ®é›†
            self.dataLoader_train = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)
            self.dataLoader_valid = DataLoader(val_dataset, batch_size=self.batch_size, shuffle=True)
            self.dataLoader_test = DataLoader(test_dataset, batch_size=self.batch_size, shuffle=True)

    def model_train(self):
        num_repeats = self.repeat_time
        # è·å–å½“å‰æ—¶é—´ï¼Œå¹¶æ ¼å¼åŒ–ä¸ºå­—ç¬¦ä¸²
        current_time = datetime.now().strftime("%Y%m%d_%H%M%S")
        save_path = os.path.join(self.data_logs_path, f"{self.model_name}-{current_time}")

        all_results = {}
        # è¾“å‡ºä¸€äº›åŸºæœ¬çš„ä¿¡æ¯
        print("æ¨¡å‹åç§°ï¼š", self.model_name)
        print("æ•°æ®é›†åç§°ï¼š", self.data_name)
        print("è®­ç»ƒè®¾å¤‡ï¼š", self.device)
        for repeat in range(1, num_repeats + 1):
            print(f"\n=======================Training Round  {repeat}  /  {self.repeat_time} =====================")
            self.model = self.initialize_model()
            criterion = self.initialize_loss()
            optimizer = self.initialize_optimizer(self.model)

            repeat_key = f"repeat_{repeat}"
            all_results[repeat_key] = {"epochs": [], "summary": {}}
            best_val_acc = 0
            best_model_state = None

            for epoch in range(1, self.epochs + 1):
                self.model.train()
                train_loss, correct, total = 0, 0, 0

                for data, labels in self.dataLoader_train:
                    data, labels = data.to(self.device), labels.to(self.device)
                    optimizer.zero_grad()
                    outputs = self.model(data)
                    loss = criterion(outputs, labels)
                    loss.backward()
                    optimizer.step()
                    train_loss += loss.item()
                    _, predicted = torch.max(outputs.data, 1)
                    total += labels.size(0)
                    correct += (predicted == labels).sum().item()

                avg_train_loss = train_loss / len(self.dataLoader_train)
                train_acc = correct / total

                val_acc, val_loss = self.model_evaluation(self.dataLoader_test, criterion)

                if val_acc > best_val_acc:
                    best_val_acc = val_acc
                    best_model_state = self.model.state_dict()

                all_results[repeat_key]["epochs"].append({
                    "epoch": epoch,
                    "train_loss": avg_train_loss,
                    "val_loss": val_loss,
                    "train_acc": train_acc,
                    "val_acc": val_acc
                })

                print(
                    f"âœ… Repeat {repeat}/{num_repeats} | ğŸ“ˆ Epoch [{epoch}/{self.epochs}] "
                    f"ğŸ”§ Train Loss: {avg_train_loss:.4f} | ğŸ¯ Train Acc: {train_acc:.4f} | "
                    f"ğŸ“Š Val Loss: {val_loss:.4f} | ğŸ’¡ Val Acc: {val_acc:.4f}"
                )

                # åŠ è½½æœ€ä½³æ¨¡å‹è¯„ä¼°æµ‹è¯•é›†
            self.model.load_state_dict(best_model_state)
            test_acc, _ = self.model_evaluation(self.dataLoader_test, criterion)
            y_true, y_pred = self.model_test(self.model, self.dataLoader_test)
            recall = recall_score(y_true, y_pred, average="macro")
            f1 = f1_score(y_true, y_pred, average="macro")

            all_results[repeat_key]["summary"] = {
                "best_val_acc": best_val_acc,
                "final_test_acc": test_acc,
                "recall": recall,
                "f1": f1
            }

            # ä¿å­˜æœ€ä¼˜æ¨¡å‹
            self.model_save(best_model_state, self.save_model_path, repeat, current_time)

        # ä¿å­˜æ•´ä¸ª JSON æ–‡ä»¶
        save_json_path = os.path.join(save_path, "logs.json")
        save_json(all_results, save_json_path)

    def model_test(self, model, dataloader):
        model.eval()
        y_true, y_pred = [], []
        with torch.no_grad():
            for data, labels in dataloader:
                data = data.to(self.device)
                outputs = model(data)
                _, predicted = torch.max(outputs.data, 1)
                y_pred.extend(predicted.cpu().numpy())
                y_true.extend(labels.numpy())
        return y_true, y_pred

    def model_evaluation(self, dataloader, criterion):

        self.model.eval()
        total_loss, correct, total = 0, 0, 0

        with torch.no_grad():
            for data, labels in dataloader:
                data, labels = data.to(self.device), labels.to(self.device)
                outputs = self.model(data)
                loss = criterion(outputs, labels)
                total_loss += loss.item()
                _, predicted = torch.max(outputs.data, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()

        avg_loss = total_loss / len(dataloader)
        acc = correct / total
        return acc, avg_loss

    def initialize_model(self):
        # å­—å…¸ç´¢å¼•æ¨¡å‹ï¼ˆè¯´å®è¯æœ‰ç‚¹å•°å—¦äº†ï¼Œä½†æ˜¯å¾ˆstrongï¼‰
        model = self.model_dict[self.model_name]().to(self.device)
        return model

    def initialize_loss(self):
        # åˆå§‹åŒ–æŸå¤±å‡½æ•°ï¼Œå‡è®¾æŸå¤±å‡½æ•°ä¸ºäº¤å‰ç†µæŸå¤±
        if self.loss_name == "CrossEntropyLoss":
            criterion = nn.CrossEntropyLoss()
        else:
            raise ValueError(f"Unsupported loss function: {self.loss_name}")
        return criterion

    def initialize_optimizer(self, model):
        # åˆå§‹åŒ–ä¼˜åŒ–å™¨ï¼Œå‡è®¾ä¼˜åŒ–å™¨ä¸º Adam
        if self.optimizer_name == "Adam":
            optimizer = torch.optim.Adam(model.parameters(), lr=self.learning_rate)
        else:
            raise ValueError(f"Unsupported optimizer: {self.optimizer_name}")
        return optimizer

    def model_save(self, model_state_dict, save_path, repeat_index, current_time):
        filename = f"best_model_repeat{repeat_index}-{current_time}.pth"
        full_path = os.path.join(save_path, filename)
        torch.save(model_state_dict, full_path)
        print(f"ğŸ’¾ Best model for Repeat {repeat_index} saved to: {full_path}")


class ModelTrainCrossValidation(ModelTrainer):
    def __init__(self, para_config_dir):
        super().__init__(para_config_dir)
        self.dataset = None

    def data_loader(self):
        if self.Subject_sigal["sigal"] == "yes":
            all_dataset = EEGDataset(self.data_name, self.data_root_path, self.Subject_sigal["subject_name"])  # åŠ è½½æ•°æ®é›†
            self.dataset = all_dataset  # å°†æ•´ä¸ªæ•°æ®é›†èµ‹å€¼ç»™ self.dataset

    def model_train(self):
        num_repeats = self.repeat_time
        k_folds = self.Cross_validation["Fload_Num"]  # è®¾ç½®kæŠ˜äº¤å‰éªŒè¯çš„æŠ˜æ•°
        kfold = KFold(n_splits=k_folds, shuffle=True)

        # è·å–å½“å‰æ—¶é—´ï¼Œå¹¶æ ¼å¼åŒ–ä¸ºå­—ç¬¦ä¸²
        current_time = datetime.now().strftime("%Y%m%d_%H%M%S")
        save_path = os.path.join(self.data_logs_path, f"{self.model_name}-{current_time}")

        all_results = {}

        for repeat in range(1, num_repeats + 1):
            print(f"\n=======================Training Round  {repeat}  /  {self.repeat_time} =====================")

            fold_results = {}
            for fold, (train_ids, test_ids) in enumerate(kfold.split(self.dataset)):
                print(f"\n----------------------- Fold {fold + 1} / {k_folds} -----------------------")

                # åˆ›å»ºæ•°æ®åŠ è½½å™¨
                train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)
                test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)

                train_loader = torch.utils.data.DataLoader(self.dataset, batch_size=self.batch_size,
                                                           sampler=train_subsampler)
                test_loader = torch.utils.data.DataLoader(self.dataset, batch_size=self.batch_size,
                                                          sampler=test_subsampler)

                self.model = self.initialize_model()
                criterion = self.initialize_loss()
                optimizer = self.initialize_optimizer(self.model)

                fold_key = f"fold_{fold + 1}"
                fold_results[fold_key] = {"epochs": [], "summary": {}}
                best_val_acc = 0
                best_model_state = None

                for epoch in range(1, self.epochs + 1):
                    self.model.train()
                    train_loss, correct, total = 0, 0, 0

                    for data, labels in train_loader:
                        data, labels = data.to(self.device), labels.to(self.device)
                        optimizer.zero_grad()
                        outputs = self.model(data)
                        loss = criterion(outputs, labels)
                        loss.backward()
                        optimizer.step()
                        train_loss += loss.item()
                        _, predicted = torch.max(outputs.data, 1)
                        total += labels.size(0)
                        correct += (predicted == labels).sum().item()

                    avg_train_loss = train_loss / len(train_loader)
                    train_acc = correct / total

                    val_acc, val_loss = self.model_evaluation(test_loader, criterion)

                    if val_acc > best_val_acc:
                        best_val_acc = val_acc
                        best_model_state = self.model.state_dict()

                    fold_results[fold_key]["epochs"].append({
                        "epoch": epoch,
                        "train_loss": avg_train_loss,
                        "val_loss": val_loss,
                        "train_acc": train_acc,
                        "val_acc": val_acc
                    })

                    print(
                        f"âœ… Repeat {repeat}/{num_repeats} | Fold {fold + 1}/{k_folds} | ğŸ“ˆ Epoch [{epoch}/{self.epochs}] "
                        f"ğŸ”§ Train Loss: {avg_train_loss:.4f} | ğŸ¯ Train Acc: {train_acc:.4f} | "
                        f"ğŸ“Š Val Loss: {val_loss:.4f} | ğŸ’¡ Val Acc: {val_acc:.4f}"
                    )

                # åŠ è½½æœ€ä½³æ¨¡å‹è¯„ä¼°æµ‹è¯•é›†
                self.model.load_state_dict(best_model_state)
                test_acc, _ = self.model_evaluation(test_loader, criterion)
                y_true, y_pred = self.model_test(self.model, test_loader)
                recall = recall_score(y_true, y_pred, average="macro")
                f1 = f1_score(y_true, y_pred, average="macro")

                fold_results[fold_key]["summary"] = {
                    "best_val_acc": best_val_acc,
                    "final_test_acc": test_acc,
                    "recall": recall,
                    "f1": f1
                }

                # ä¿å­˜æœ€ä¼˜æ¨¡å‹
                self.model_save(best_model_state, self.save_model_path, repeat, current_time, fold + 1)

            all_results[f"repeat_{repeat}"] = fold_results

        # ä¿å­˜æ•´ä¸ª JSON æ–‡ä»¶
        save_json_path = os.path.join(save_path, "logs.json")
        save_json(all_results, save_json_path)

    def model_save(self, model_state_dict, save_path, repeat_index, current_time, fold):
        filename = f"best_model_repeat{repeat_index}-{fold}-{current_time}.pth"
        full_path = os.path.join(save_path, filename)
        torch.save(model_state_dict, full_path)
        print(f"ğŸ’¾ Best model for Repeat {repeat_index} saved to: {full_path}")
